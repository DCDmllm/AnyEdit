{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from skimage import morphology\n",
    "import mediapipe as mp\n",
    "import warnings\n",
    "import torch\n",
    "import cv2\n",
    "from torchvision.transforms import Compose\n",
    "from pathlib import Path\n",
    "lama_path = Path(__file__).resolve().parent.parent.parent / \"AnyEdit_Collection/other_modules\"\n",
    "sys.path.insert(0, str(lama_path))\n",
    "from AnyEdit_Collection.other_modules.depth_anything_v2.dpt import DepthAnythingV2\n",
    "import matplotlib\n",
    "import glob\n",
    "from AnyEdit_Collection.other_modules.DPT.util import io\n",
    "from AnyEdit_Collection.other_modules.DPT.dpt.transforms import Resize, NormalizeImage, PrepareForNet\n",
    "from termcolor import cprint\n",
    "from AnyEdit_Collection.other_modules.uniformer.mmseg.datasets.pipelines import Compose\n",
    "from AnyEdit_Collection.other_modules.uniformer.mmseg.apis import init_segmentor, inference_segmentor, show_result_pyplot\n",
    "from AnyEdit_Collection.other_modules.uniformer.mmseg.core.evaluation import get_palette\n",
    "from AnyEdit_Collection.other_modules.HED import HEDdetector\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def img2sketch(image_path, output_path):\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # Apply Gaussian blur\n",
    "    gaussian_blur = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "    # Apply edge detection\n",
    "    edges = cv2.Canny(gaussian_blur, 50, 150)\n",
    "    # Invert the colors of the edges\n",
    "    edges = cv2.bitwise_not(edges)\n",
    "    # Save the result\n",
    "    cv2.imwrite(output_path, edges)\n",
    "\n",
    "def run_depth(input_image, output_image, model):\n",
    "    \"\"\"Run MonoDepthNN to compute depth maps.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): path to input image\n",
    "        output_path (str): path to output image\n",
    "        model_path (str): path to saved model\n",
    "    \"\"\"\n",
    "\n",
    "    # select device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    net_w = net_h = 384\n",
    "    normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    transform = Compose(\n",
    "        [\n",
    "            Resize(\n",
    "                net_w,\n",
    "                net_h,\n",
    "                resize_target=None,\n",
    "                keep_aspect_ratio=True,\n",
    "                ensure_multiple_of=32,\n",
    "                resize_method=\"minimal\",\n",
    "                image_interpolation_method=cv2.INTER_CUBIC,\n",
    "            ),\n",
    "            normalization,\n",
    "            PrepareForNet(),\n",
    "        ]\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        model = model.to(memory_format=torch.channels_last)\n",
    "        model = model.half()\n",
    "\n",
    "    model.to(device)\n",
    "    img = io.read_image(input_image)\n",
    "\n",
    "    img_input = transform({\"image\": img})[\"image\"]\n",
    "\n",
    "    # compute\n",
    "    with torch.no_grad():\n",
    "        sample = torch.from_numpy(img_input).to(device).unsqueeze(0)\n",
    "\n",
    "        if device == torch.device(\"cuda\"):\n",
    "            sample = sample.to(memory_format=torch.channels_last)\n",
    "            sample = sample.half()\n",
    "\n",
    "        prediction = model.forward(sample)\n",
    "        prediction = (\n",
    "            torch.nn.functional.interpolate(\n",
    "                prediction.unsqueeze(1),\n",
    "                size=img.shape[:2],\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "            .squeeze()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "    cprint(output_image, 'red')\n",
    "    io.write_depth(output_image, prediction, bits=2, absolute_depth=False)\n",
    "\n",
    "\n",
    "def img2depth(image_path, output_path, depth_anything = None):\n",
    "    '''\n",
    "    change to depth anything V2\n",
    "    '''\n",
    "    \n",
    "\n",
    "    if os.path.isfile(image_path):\n",
    "        if image_path.endswith('txt'):\n",
    "            with open(image_path, 'r') as f:\n",
    "                filenames = f.read().splitlines()\n",
    "        else:\n",
    "            filenames = [image_path]\n",
    "    else:\n",
    "        filenames = glob.glob(os.path.join(image_path, '**/*'), recursive=True)\n",
    "\n",
    "    cmap = matplotlib.colormaps.get_cmap('Spectral_r')\n",
    "\n",
    "    for k, filename in enumerate(filenames):\n",
    "        raw_image = cv2.imread(filename)\n",
    "        depth = depth_anything.infer_image(raw_image)\n",
    "        depth = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0\n",
    "        depth = depth.astype(np.uint8)\n",
    "        depth = (cmap(depth)[:, :, :3] * 255)[:, :, ::-1].astype(np.uint8)\n",
    "        cv2.imwrite(output_path, depth)\n",
    "\n",
    "\n",
    "def img2seg(image_path, output_path, model):\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "    result = inference_segmentor(model, img)\n",
    "    res_img = show_result_pyplot(model, img, result, get_palette('ade'), opacity=1)\n",
    "    # Save the result\n",
    "    cv2.imwrite(output_path, res_img)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model = init_segmentor(config='./AnyEdit_Collection/other_modules/uniformer', device='cuda',\n",
    "                               checkpoint='./checkpoints/visual_models/annotator/ckpts/upernet_global_small.pth')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from AnyEdit_Collection.adaptive_editing_pipelines.tools.tool import prepare_output_dir\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "task = 'segment'\n",
    "action = 'remove'\n",
    "\n",
    "# place to hold edited images\n",
    "sketch_root = f'./AnyEdit/data/{action}/edited_img'\n",
    "\n",
    "# place to hold segment\n",
    "output_dir = f'./AnyEdit/data/visual_{task}'\n",
    "prepare_output_dir(output_dir)\n",
    "\n",
    "# successful instructions\n",
    "json_path = f'./AnyEdit/data/{action}/edit_success_-1_-1.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    instructions = json.load(f)\n",
    "\n",
    "# final results\n",
    "final_instructions = []\n",
    "if os.path.exists(os.path.join(output_dir,'edit_result.json')):\n",
    "    with open(os.path.join(output_dir,'edit_result.json'), 'r') as f:\n",
    "        final_instructions = json.load(f)\n",
    "data = {}\n",
    "iter = 0\n",
    "\n",
    "# for image in os.listdir(scribble_root):\n",
    "for ins in tqdm(instructions):\n",
    "    \n",
    "    iter += 1\n",
    "    ins['edit_type'] = 'sketch'\n",
    "    ins['edit'] = random.choice(['Follow ', 'Refer to ', 'Watch ']) + f'the given sketch [V*] to {action} {ins[\"edited object\"]} '\n",
    "    image_path = os.path.join(sketch_root, ins['image_file'])\n",
    "    \n",
    "    if not os.path.exists(image_path):\n",
    "        print(f'{image_path} not exists')\n",
    "        continue\n",
    "    \n",
    "    output_path = os.path.join(output_dir, 'edited_img',ins['image_file'])\n",
    "\n",
    "    img2seg(image_path=image_path,\n",
    "                output_path=output_path, model=seg_model)\n",
    "    final_instructions.append(ins)\n",
    "    if iter % 1000 == 0:\n",
    "        with open(os.path.join(output_dir,'edit_result.json'), 'w') as f:\n",
    "            json.dump(final_instructions, f)\n",
    "with open(os.path.join(output_dir,'edit_result.json'), 'w') as f:\n",
    "            json.dump(final_instructions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Scribble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hed_model = HEDdetector(path='./checkpoints/visual_models/ControlNetHED.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from AnyEdit_Collection.adaptive_editing_pipelines.tools.tool import prepare_output_dir\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "task = 'scribble'\n",
    "action = 'remove'\n",
    "\n",
    "# place to hold edited images\n",
    "sketch_root = f'./AnyEdit/data/{action}/edited_img'\n",
    "\n",
    "# place to hold scribbles\n",
    "output_dir = f'./AnyEdit/data/visual_{task}'\n",
    "prepare_output_dir(output_dir)\n",
    "\n",
    "# successful instructions\n",
    "json_path = f'./AnyEdit/data/{action}/edit_success_-1_-1.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    instructions = json.load(f)\n",
    "\n",
    "# final results\n",
    "final_instructions = []\n",
    "if os.path.exists(os.path.join(output_dir,'edit_result.json')):\n",
    "    with open(os.path.join(output_dir,'edit_result.json'), 'r') as f:\n",
    "        final_instructions = json.load(f)\n",
    "data = {}\n",
    "iter = 0\n",
    "\n",
    "# for image in os.listdir(scribble_root):\n",
    "for ins in tqdm(instructions):\n",
    "    \n",
    "    iter += 1\n",
    "    ins['edit_type'] = 'sketch'\n",
    "    ins['edit'] = random.choice(['Follow ', 'Refer to ', 'Watch ']) + f'the given sketch [V*] to {action} {ins[\"edited object\"]} '\n",
    "    image_path = os.path.join(sketch_root, ins['image_file'])\n",
    "    \n",
    "    if not os.path.exists(image_path):\n",
    "        image_path = image_path.replace('.jpg', '_0.png')\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f'{image_path} not exists')\n",
    "        continue\n",
    "    \n",
    "    output_path = os.path.join(output_dir, 'edited_img',ins['image_file'])\n",
    "\n",
    "    hed_model(image_path, output_path)\n",
    "    final_instructions.append(ins)\n",
    "with open(os.path.join(output_dir,'edit_result.json'), 'w') as f:\n",
    "    json.dump(final_instructions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from AnyEdit_Collection.adaptive_editing_pipelines.tools.tool import prepare_output_dir\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "task = 'sketch'\n",
    "action = 'remove'\n",
    "\n",
    "# place to hold edited images\n",
    "sketch_root = f'./AnyEdit/data/{action}/edited_img'\n",
    "\n",
    "# place to hold sketch\n",
    "output_dir = f'./AnyEdit/data/visual_{task}'\n",
    "prepare_output_dir(output_dir)\n",
    "\n",
    "# successful instructions\n",
    "json_path = f'./AnyEdit/data/{action}/edit_success_-1_-1.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    instructions = json.load(f)\n",
    "\n",
    "# final results\n",
    "final_instructions = []\n",
    "if os.path.exists(os.path.join(output_dir,'edit_result.json')):\n",
    "    with open(os.path.join(output_dir,'edit_result.json'), 'r') as f:\n",
    "        final_instructions = json.load(f)\n",
    "data = {}\n",
    "iter = 0\n",
    "action = 'remove'\n",
    "# for image in os.listdir(scribble_root):\n",
    "for ins in tqdm(instructions):\n",
    "    \n",
    "    iter += 1\n",
    "    ins['edit_type'] = 'sketch'\n",
    "    ins['edit'] = random.choice(['Follow ', 'Refer to ', 'Watch ']) + f'the given sketch [V*] to {action} {ins[\"edited object\"]} '\n",
    "    image_path = os.path.join(sketch_root, ins['image_file'])\n",
    "    \n",
    "    if not os.path.exists(image_path):\n",
    "        image_path = image_path.replace('.jpg', '_0.png')\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f'{image_path} not exists')\n",
    "        continue\n",
    "    \n",
    "    output_path = os.path.join(output_dir, 'edited_img',ins['image_file'])\n",
    "    output_path = output_path.replace('jpg', 'png')\n",
    "    img2sketch(image_path=image_path,\n",
    "                output_path=output_path)\n",
    "    final_instructions.append(ins)\n",
    "    if iter % 1000 == 0:\n",
    "        with open(os.path.join(output_dir,'edit_result.json'), 'w') as f:\n",
    "            json.dump(final_instructions, f)\n",
    "with open(os.path.join(output_dir,'edit_result.json'), 'w') as f:\n",
    "            json.dump(final_instructions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "depth_anything = DepthAnythingV2(encoder='vitl', features=256, out_channels=[256, 512, 1024, 1024])\n",
    "depth_anything.load_state_dict(\n",
    "    torch.load(f'./checkpoints/visual_models/depth_anything_v2/depth_anything_v2_vitl.pth',\n",
    "                map_location='cpu'))\n",
    "depth_anything = depth_anything.to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from AnyEdit_Collection.adaptive_editing_pipelines.tools.tool import prepare_output_dir\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "task = 'depth'\n",
    "action = 'remove'\n",
    "\n",
    "# place to hold edited images\n",
    "depth_root = f'./AnyEdit/data/{action}/edited_img'\n",
    "\n",
    "# place to hold depth\n",
    "output_dir = f'./AnyEdit/data/visual_{task}'\n",
    "prepare_output_dir(output_dir)\n",
    "\n",
    "# successful instructions\n",
    "json_path = f'./AnyEdit/data/{action}/edit_success_-1_-1.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    instructions = json.load(f)\n",
    "\n",
    "# final results\n",
    "final_instructions = []\n",
    "if os.path.exists(os.path.join(output_dir,'edit_result.json')):\n",
    "    with open(os.path.join(output_dir,'edit_result.json'), 'r') as f:\n",
    "        final_instructions = json.load(f)\n",
    "data = {}\n",
    "iter = 0\n",
    "\n",
    "# for image in os.listdir(scribble_root):\n",
    "for ins in tqdm(instructions):\n",
    "    \n",
    "    iter += 1\n",
    "    ins['edit_type'] = 'depth image'\n",
    "    ins['edit'] = random.choice(['Follow ', 'Refer to ', 'Watch ']) + f'the given depth image [V*] to {action} {ins[\"edited object\"]} '\n",
    "    image_path = os.path.join(sketch_root, ins['image_file'])\n",
    "    \n",
    "    if not os.path.exists(image_path):\n",
    "        image_path = image_path.replace('.jpg', '_0.png')\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f'{image_path} not exists')\n",
    "        continue\n",
    "    \n",
    "    output_path = os.path.join(output_dir, 'edited_img',ins['image_file'])\n",
    "    output_path = output_path.replace('jpg', 'png')\n",
    "    img2depth(image_path=image_path,\n",
    "                output_path=output_path, depth_anything = depth_anything)\n",
    "    final_instructions.append(ins)\n",
    "    if iter % 1000 == 0:\n",
    "        with open(os.path.join(output_dir,'edit_result.json'), 'w') as f:\n",
    "            json.dump(final_instructions, f)\n",
    "with open(os.path.join(output_dir,'edit_result.json'), 'w') as f:\n",
    "            json.dump(final_instructions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bbox 区域"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "import torch\n",
    "from PIL import Image\n",
    "import random\n",
    "import GroundingDINO.groundingdino.datasets.transforms as T\n",
    "from GroundingDINO.groundingdino.models import build_model\n",
    "from GroundingDINO.groundingdino.util.slconfig import SLConfig\n",
    "from GroundingDINO.groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from segment_anything import build_sam, SamPredictor\n",
    "import cv2\n",
    "import numpy as np\n",
    "import warnings\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import spacy\n",
    "# from tool import is_human_variant, return_parameters, maskgeneration\n",
    "from termcolor import cprint  \n",
    "from AnyEdit_Collection.adaptive_editing_pipelines.tools.tool import maskgeneration, get_bbox_from_mask     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tool_model():\n",
    "    config_file = './GroundingDINO/groundingdino/config/GroundingDINO_SwinB_cfg.py' #'GroundingDINO/groundingdino/config/GroundingDINO_SwinB_cfg.py' \n",
    "    grounded_checkpoint = './checkpoints/foundation_models/groundingDINO/groundingdino_swinb_cogcoor.pth' #'checkpoints/groundingdino_swinb_cogcoor.pth'  \n",
    "    sam_checkpoint = './checkpoints/foundation_models/sam_vit_h_4b8939.pth' #'checkpoints/sam_vit_h_4b8939.pth'\n",
    "    \n",
    "    device = 'cuda'\n",
    "\n",
    "    det_model = load_model(config_file, grounded_checkpoint, device=device)\n",
    "    sam_model = SamPredictor(build_sam(checkpoint=sam_checkpoint).to(device))\n",
    "\n",
    "   \n",
    "    return det_model, sam_model\n",
    "def load_model(model_config_path, model_checkpoint_path, device):\n",
    "    args = SLConfig.fromfile(model_config_path)\n",
    "    args.device = device\n",
    "    model = build_model(args)\n",
    "    checkpoint = torch.load(model_checkpoint_path, map_location=\"cuda\")\n",
    "    load_res = model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
    "    _ = model.eval()\n",
    "    return model\n",
    "\n",
    "def draw_bbox(image, output_path, bbox):\n",
    "    if isinstance(image, str):\n",
    "        image = cv2.imread(image_path)\n",
    "    \n",
    "    cv2.rectangle(image, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 0, 255), 2)\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    cv2.imwrite(output_path, image)\n",
    "    \n",
    "def img2bbox(input_path, output_path, target_obj, det_model, sam_model):\n",
    "    image = cv2.imread(input_path)\n",
    "    mask_pil,image_pil,bbox_pil,union = maskgeneration(det_model, sam_model, input_path, target_obj)\n",
    "    if mask_pil is None:\n",
    "        print(f'Can not find {target_obj} in {input_path}')\n",
    "        return False, None\n",
    "    y1, y2, x1, x2 = get_bbox_from_mask(np.array(mask_pil))\n",
    "    image_pil = np.array(image_pil)\n",
    "    draw_bbox(image_pil, output_path, [x1, y1, x2, y2])\n",
    "    \n",
    "    return True, mask_pil\n",
    "\n",
    "def draw_bbox(image, output_path, bbox):\n",
    "    if isinstance(image, str):\n",
    "        image = cv2.imread(image_path)\n",
    "    \n",
    "    cv2.rectangle(image, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 0, 255), 2)\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    cv2.imwrite(output_path, image)\n",
    "    \n",
    "def img2bbox(input_path, output_path, target_obj, det_model, sam_model):\n",
    "    image = cv2.imread(input_path)\n",
    "    mask_pil,image_pil,bbox_pil,union = maskgeneration(det_model, sam_model, input_path, target_obj)\n",
    "    if mask_pil is None:\n",
    "        print(f'Can not find {target_obj} in {input_path}')\n",
    "        return False, None\n",
    "    y1, y2, x1, x2 = get_bbox_from_mask(np.array(mask_pil))\n",
    "    image_pil = np.array(image_pil)\n",
    "    draw_bbox(image_pil, output_path, [x1, y1, x2, y2])\n",
    "    \n",
    "    return True, mask_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_model, sam_model = load_tool_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from AnyEdit_Collection.adaptive_editing_pipelines.tools.tool import prepare_output_dir\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "task = 'boundingbox'\n",
    "action = 'remove'\n",
    "\n",
    "# place to hold edited images\n",
    "boundingbox_root = f'./AnyEdit/data/{action}/edited_img'\n",
    "\n",
    "# place to hold boundingbox\n",
    "output_dir = f'./AnyEdit/data/visual_{task}'\n",
    "prepare_output_dir(output_dir)\n",
    "\n",
    "# successful instructions\n",
    "json_path = f'./AnyEdit/data/{action}/edit_success_-1_-1.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    instructions = json.load(f)\n",
    "\n",
    "# final results\n",
    "final_instructions = []\n",
    "if os.path.exists(os.path.join(output_dir,'edit_result.json')):\n",
    "    with open(os.path.join(output_dir,'edit_result.json'), 'r') as f:\n",
    "        final_instructions = json.load(f)\n",
    "pre_state= 0\n",
    "if os.path.exists(os.path.join(output_dir,'state.json')):\n",
    "    with open(os.path.join(output_dir,'state.json'), 'r') as f:\n",
    "        data = json.load(f)\n",
    "        pre_state = data['iter']\n",
    "data = {}\n",
    "\n",
    "# for image in os.listdir(scribble_root):\n",
    "for ins in tqdm(instructions):\n",
    "    iter+=1\n",
    "    if iter < pre_state:\n",
    "        continue\n",
    "    \n",
    "    ins['edit_type'] = 'visual_boundingbox'\n",
    "    ins['edit'] = random.choice(['Follow ', 'Refer to ', 'Watch ']) + f'the given boundingbox [V*] to change {ins[\"edited object\"]} '\n",
    "    image_path = os.path.join(root, ins['image_file'])\n",
    "    image_path = image_path.replace('jpg', 'png')\n",
    "    output_path = os.path.join(output_dir, 'edited_img',ins['image_file'])\n",
    "    output_path = output_path.replace('jpg', 'png')\n",
    "    success,mask = img2bbox(input_path=image_path,\n",
    "                output_path=output_path, target_obj = ins['ref_object'], det_model = det_model, sam_model = sam_model)\n",
    "    if success:\n",
    "        final_instructions.append(ins)\n",
    "    else:\n",
    "        print(f'Can not find {ins[\"edited object\"]} in {image_path}')\n",
    "    if iter % 50 == 0:\n",
    "        with open(os.path.join(output_dir,'edit_result.json'), 'w') as f:\n",
    "            json.dump(final_instructions, f)\n",
    "        with open(os.path.join(output_dir,'state.json'), 'w') as f:\n",
    "            json.dump({'iter':iter}, f)\n",
    "        \n",
    "        \n",
    "with open(os.path.join(output_dir,'edit_result.json'), 'w') as f:\n",
    "        json.dump(final_instructions, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anyedit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
